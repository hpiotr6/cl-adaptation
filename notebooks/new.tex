\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Variance-Covariance Regularization Improves Continual Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\newcommand{\ours}{VCRCL}

\newcommand\ph[1]{{\color{magenta} [PH: #1]}}
\newcommand\dm[1]{{\color{green} [DM: #1]}}
\newcommand\kd[1]{{\color{magenta}  KD: #1}}
\newcommand\TODO[1]{{\color{red} [ TODO: #1]}}


\begin{document}


\maketitle


\begin{abstract}
  In this work, we explore the benefits of Variance-Covariance Regularization in Continual Learning (CL). Neural networks suffer from abrupt performance loss when retrained with additional data. Numerous CL approaches try to mitigate this problem by applying various regularization techniques that limit the update of the model parts that were crucial for the previous tasks. In this work, we propose to look at this problem from the different perspective, and explore how we can prepare the continually trained model for future changes. We propose to employ a variance-covariance regularization that improves classifier robustness, and hence enhance its immunity to the problem of catastrophic forgetting. In our experiments we combine our method with various CL techniques and show that it provides significant performance gain across different datasets, training scenarios and architectures.
\end{abstract}


\section{Introduction}

As machine learning applications proliferate across various domains, the demand for systems capable of adapting to the evolving environments becomes increasingly evident. However, recent neural networks suffer from problem known as \emph{catastrophic forgetting}~\cite{1999french} when retrained on additional tasks. Different continual learning methods try to overcome this issue. One of the promising CL technique is to regularize the model during the update on a new task, in order to limit the interference with previously encoded knowledge. 

In particular, Elastic Weight Consolidation (EWC)~\cite{kirkpatrick2017overcoming} mitigate forgetting by penalizing update of parameters important for prior tasks according to the Fisher information matrix. Similarly Memory Aware Synapses (MAS)~\cite{aljundi2017memory} Synaptic Intelligence (SI)~\cite{zenke2017continual} ,introduce specific neurons that store additional information about their importance for previous tasks. In general, the aim of recent regularization techniques is to restrict the model update on the new task, by taking into account information from the previous ones. In this work, we propose a conceptually different approach and explore the possibility of regularizing the neural network on a given task in order to prepare it for changes that might happen because of the update on future tasks.

In particular, we show that by employing a variance-covariance regularization~\cite{zhu2023variancecovariance}, we can increase the robustness of the classifier in a way that facilitates continual learning. In our experiments we show that mitigating neural collapse by enforcing high variance and low covariance of data features leads to the creation of models that \kd{SMTH :D}. 

Based on those experiments, we propose to combine our method with known continual learning approaches as an additional \emph{plug-in}. We evaluate this idea with different methods (regularization, architectural and replay), scenarios (equal/diverse size-splits), and architectures (resnet, convnext). We can observe increased performance in almost all evaluated combinations, while for some setups the gain from our regularization is instrumental.

The contribution of this work can be summarized in three points:
\begin{itemize}
    \item We analyse the variance-covariance regularization and highlight its potential in continual learning
    \item We introduce a VCCL plug-in that can be added to different continual-learning techniques
    \item We extensively evaluate our approach and show that it improves the performance of different CL techniques independently of the selected dataset, scenario or model architecture.
\end{itemize}

\section{Related Works}
The problem of continual learning attracts the increasing attention of the research community~\cite{parisi2019continual,mundt2023wholistic}, which results in the growing number of novel methods. The primary goal of those approaches is to mitigate catastrophic forgetting~\cite{1999french}, while at the same time allowing for model update with additional data. Usually CL methods are divided into three main groups: (1) Regularization methods (e.g. EWC~\cite{kirkpatrick2017overcoming}, LwF~\cite{li2017learning} try to limit the update of the model for the neurons that are crucial for previous tasks. (2)  Architecture-based approaches (e.g.RCL~\cite{2018xu+1},PNN~\cite{2016rusu+7}) adapt only the task-specific part of the model, while (3) replay techniques train the model on new dataset, while mixing it with additional rehearsal samples coming from the memory buffer~\cite{2019rolnick+4,aljundi2019online} or generative model~\cite{2017shin+3}.

\paragraph{Regularization in CL}

\paragraph{Selecting update directions in CL}
Nullspaces, orthogonalization

\paragraph{Robustness in CL? + Other plugins}

\paragraph{Neural collapse reduction in NN}

\section{Motivation}

\section{Method}

\subsection{Problem Statement}
% https://arxiv.org/pdf/2210.06443
A Class-Incremental Learning (CIL) problem involves learning a function $f$ from a stream of data, which we formalize as a sequence of disjoint datasets $T=\{\tau_0, \tau_1, \ldots, \tau_{|T|}\}$, where $\tau_t = \{(x_i, y_i)\}_{i=1}^{N_t}$ and $\tau_i\cap\tau_j=\varnothing$; the label set $Y_t$ for each $\tau_t$ are non-overlapping. In this setting, the ideal objective consists in minimizing the overall loss over all tasks experienced, formally:
\begin{equation}
    f^*=\argmin_{f}{\mathbb{E}_{t=0}^{|T|}{\bigg{[}\mathbb{E}_{(x,y)\sim\tau_t}{\Big{[}\mathcal{L}(f(x),y)\Big{]}}\bigg{]}}},
\end{equation}
where $\mathcal{L}$ is an appropriate loss for solving the task at hand. In a continual scenario, only data from the current task $\tau_t$ is freely available; therefore, CL methods need to maintain knowledge from the past $t-1$ tasks in order to solve the overall problem.


\subsection{Variance-Covariance Regularization for Continual Learning}
% https://arxiv.org/pdf/2306.13292
At each intermediate layer $j$, we denote the feature representation of an input $x_i$ as $z_i^{(j)} \in R^{D_j}$. 

We apply VCCL regularization on $K$ selected layers. Firstly, we center the representations of a given batch, \textit{i.e.} $z'^{(j)} = z^{(j)} - \bar{z}^{(j)}$. Note, that we alter the forward pass of the network by using $z'^{(j)}$ as an input to the $j+1$-th layer instead of using $z^{(j)}$. Then, for each selected layer indexed by $k \in K$, we calculate the variance and covariance loss functions are defined as:
\begin{align}
\ell_{\mathrm{var}} (j) &= \frac{1}{D_j} \sum_{m=1}^{D_j} \max(0, 1 - \sqrt{C_{mm}}) \label{eq:var}\\
\ell_{\mathrm{cov}} (j) &= \frac{1}{D_j(D_j-1)} \sum_{m \neq n} C_{mn}^2 \label{eq:cov}
\end{align}
where \(C = \frac{1}{D_j-1}\sum_{i=1}^{D_j} z'^{(j)}_i z'^{(j)T}_i \) denotes the covariance matrix. We aggregate the layer-wise losses:
\begin{equation}
    \mathcal{L}_{vc} = \sum_{k \in K} (\alpha l_{var}(k) + \beta l_{cov}(k))
\end{equation}
where $\alpha$ and $\beta$ control the strength of variance and covariance regularization, respectively. 

The overall objective combines loss of the base CL method and VCCL plug-in:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{base}} + \mathcal{L}_{vc}
\end{equation}

\section{Experiments}

\TODO{Piotrek}


\subsection{Experimental setup}


\subsection{Main results}

% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{llll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[h]
%   \centering
%     \begin{tabular}{lcccccl}\toprule
%     & \multicolumn{3}{c}{A} & \multicolumn{3}{c}{B}
%     \\\cmidrule(lr){2-4}\cmidrule(lr){5-7}
%                & $mv$  & Rel.~err & Time    & $mv$  & Rel.~err & Time\\\midrule
%     trigmv    & 11034 & 1.3e-7 & 3.9 & 15846 & 2.7e-11 & 5.6 \\
%     trigexpmv & 21952 & 1.3e-7 & 6.2 & 31516 & 2.7e-11 & 8.8 \\
%     trigblock & 15883 & 5.2e-8 & 7.1 & 32023 & 1.1e-11 & 1.4e1\\
%     expleja   & 11180 & 8.0e-9 & 4.3 & 17348 & 1.5e-11 & 6.6 \\\bottomrule
%     \end{tabular}
% \end{table}
\newcommand\tab[1][10pt]{\hspace*{#1}}

\begin{table}[h]
  \caption{Task Agnostic results using ResNet-34 backbone}
  \label{sample-table}
  \centering
  \begin{tabular}{llllllll}
    \toprule
    Dataset & \multicolumn{3}{c}{CIFAR100} & \multicolumn{2}{c}{ImageNet100} \\
    Num tasks & 5 & 10 & 20 & 10 & 20\\
    % Method & Tag Acc \\
    \midrule
    Finetuning             &  20.88          &  11.69          &  5.25           &  11.56          &  6.38            \\
    \tab + vcreg           &  28.87          &  13.37          &  7.44           &  12.8           &  6.32            \\
    \tab  $\Delta$         &  \textbf{+7.99} &  \textbf{+1.68} &  \textbf{+2.19} &  \textbf{+1.24} &  -0.06           \\\hline
    Finetuning + exemplars &  41.24          &  37.5           &  33.43          &  37.6           &  37.46           \\
    \tab + vcreg           &  43.73          &  37.2           &  32.4           &  41.26          &  38.00           \\
    \tab  $\Delta$         &  \textbf{+2.49} &  -0.3           &  -1.03          &  \textbf{+3.66} &  \textbf{+0.54}  \\\hline
    EWC                    &  23.69          &  12.19          &  6.32           &  13.52          &  6.96            \\
    \tab + vcreg           &  32.98          &  17.25          &  11.33          &  17.24          &  7.16            \\ %dla demean by≈Ço 9.xx
    \tab  $\Delta$         &  \textbf{+9.29} &  \textbf{+5.06 }&  \textbf{+5.01} &  \textbf{+3.72} &  \textbf{0.20}   \\\hline
    LWF                    &  39.45          &  26.45          &  17.62          &  34.18          &  20.14           \\
    \tab + vcreg           &  50.39          &  38.2           &  20.82          &  38.16          &  20.86           \\
    \tab  $\Delta$         &  \textbf{+10.94}& \textbf{+11.75} & \textbf{+3.2}   &  \textbf{+3.98} &  \textbf{+0.72}   \\
    FeTRIL                    &      \\
    \tab + vcreg           &    \\
    \tab  $\Delta$         &  \\
    SMTH                    &      \\
    \tab + vcreg           &    \\
    \tab  $\Delta$         &  \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[h]
  \caption{convnext tiny}
  \label{sample-table}
  \centering
   \begin{tabular}{llllllll}
    \toprule
    Dataset & \multicolumn{3}{c}{CIFAR100} & \multicolumn{2}{c}{ImageNet100} \\
    Num tasks & 5 & 10 & 20 & 10 & 20\\
    \midrule
    Finetuning             &  24.99 & 14.14 & 7.44 & 17.38 & 7.36   \\
    \tab + vcreg           &  31.36  & 20.05 & 13.76 & 20.28 & 14.02   \\
    \tab  $\Delta$         & \textbf{ +6.37}  & \textbf{+5.91} & \textbf{+6.32} & \textbf{+2.9} & \textbf{+6.66}   \\\hline
    Finetuning + exemplars &  33.71 & 31.02 & 23.43 & 34.98 & x   \\
    \tab + vcreg           &  39.36  & 34.78 & 29.59 & 29.78 & x   \\
    \tab  $\Delta$         &  \textbf{+5.65}  & \textbf{+3.76} & \textbf{+6.16} & -5.2 & x   \\\hline
    EWC                    &  28.24  & 17.13 & 9.73 & 21.2 & x   \\
    \tab + vcreg           &  34.53  & 22.94 & 15.52 & 26.78 & x   \\
    \tab  $\Delta$         &  \textbf{+6.29}  & \textbf{+5.81} & \textbf{+5.79} & \textbf{+5.58} & x   \\\hline
    LWF                    &  39.61  & 29.35 & 22.13 & ? & x   \\
    \tab + vcreg           &  49.74  & 39.98 & 31.24 & ? & x   \\
    \tab  $\Delta$         &  \textbf{+4.13}  & \textbf{+10.63} & \textbf{+9.11} & x & x   \\
    FeTRIL                 &         & x & x & x & x   \\
    \tab + vcreg           &  x  & x & x & x & x   \\
    \tab  $\Delta$         & x & x & x & x & x   \\
    SMTH                    &  x & x & x & x & x   \\
    \tab + vcreg           & x  & x & x & x & x   \\
    \tab  $\Delta$         & x & x & x & x & x   \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Extended analysis}


% \clearpage
% \input{src/instructions}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{bibliography.bib}

\end{document}