{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2d8065-9ed8-4f91-bb80-94aeb3184eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_tiny\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pprint import pprint\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "938246ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1377e+00, -3.3865e+00,  1.3733e+00, -3.8170e-01, -3.4974e+00,\n",
       "          6.1545e-01, -1.4325e-01,  2.2090e+00,  3.8183e+00,  4.6205e+00,\n",
       "          5.3637e+00, -3.4302e-01, -3.7903e+00, -6.6501e+00, -4.1812e+00,\n",
       "          1.7244e+00,  1.5480e+00, -1.2164e+00,  2.9571e+00, -2.3404e+00],\n",
       "        [-4.5956e+00, -2.4892e+00,  2.1531e+00, -1.0826e+00, -4.4620e+00,\n",
       "          1.3942e+00,  6.5084e-02,  2.3124e+00,  5.2169e+00,  3.5880e+00,\n",
       "          4.0623e+00,  1.3788e+00, -3.9375e+00, -4.0590e+00, -3.0999e+00,\n",
       "         -3.5504e-01,  2.1612e+00, -3.0721e+00,  4.1528e+00, -4.1632e+00],\n",
       "        [-2.7417e+00, -5.9692e+00,  3.9270e+00, -2.1274e+00, -5.1613e+00,\n",
       "         -4.0790e+00,  4.6487e-01,  1.1927e+00,  3.6424e+00,  5.4612e+00,\n",
       "          2.0152e+00,  5.7825e+00, -4.6513e+00, -3.1902e+00, -1.6405e+00,\n",
       "          1.0753e+00,  4.5187e+00, -4.8021e+00,  3.7903e+00, -2.4267e+00],\n",
       "        [-3.4256e+00, -2.0507e+00,  1.5355e+00, -9.0596e-01, -2.3471e+00,\n",
       "         -4.0159e-01,  2.1646e+00,  4.4311e+00,  4.1550e+00, -1.9799e-01,\n",
       "          2.5580e+00, -5.0560e-01, -6.1032e+00, -5.5387e+00,  2.5903e-01,\n",
       "         -3.1340e-03,  1.3801e-03, -2.4734e+00,  5.3485e+00, -1.7518e+00],\n",
       "        [-3.2510e+00, -4.7877e+00,  2.7226e+00, -2.6364e+00, -5.1367e+00,\n",
       "         -2.1369e+00,  1.4716e+00,  4.1058e+00,  3.5535e+00,  5.6072e+00,\n",
       "          1.9086e+00,  4.2951e+00, -4.8667e+00, -3.3175e+00, -8.7557e-01,\n",
       "          5.4852e-01,  2.6105e+00, -4.5392e+00,  5.5093e+00, -4.3654e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.rand((5, 3, 32, 32))\n",
    "for name, module in model.convnet.features.named_children():\n",
    "    module.register_forward_hook(lambda module, input, output: o.append(output.shape))\n",
    "model(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4cf4e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 96, 34, 34]),\n",
       " torch.Size([5, 96, 34, 34]),\n",
       " torch.Size([5, 192, 17, 17]),\n",
       " torch.Size([5, 192, 17, 17]),\n",
       " torch.Size([5, 384, 8, 8]),\n",
       " torch.Size([5, 384, 8, 8]),\n",
       " torch.Size([5, 768, 4, 4]),\n",
       " torch.Size([5, 768, 4, 4]),\n",
       " torch.Size([5, 96, 8, 8]),\n",
       " torch.Size([5, 96, 8, 8]),\n",
       " torch.Size([5, 192, 4, 4]),\n",
       " torch.Size([5, 192, 4, 4]),\n",
       " torch.Size([5, 384, 2, 2]),\n",
       " torch.Size([5, 384, 2, 2]),\n",
       " torch.Size([5, 768, 1, 1]),\n",
       " torch.Size([5, 768, 1, 1])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bd7878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([5, 96, 34, 34]),\n",
      " torch.Size([5, 96, 34, 34]),\n",
      " torch.Size([5, 192, 17, 17]),\n",
      " torch.Size([5, 192, 17, 17]),\n",
      " torch.Size([5, 384, 8, 8]),\n",
      " torch.Size([5, 384, 8, 8]),\n",
      " torch.Size([5, 768, 4, 4]),\n",
      " torch.Size([5, 768, 4, 4])]\n",
      "Found convolutional layer: features.0.0, Original kernel size: (3, 3)\n",
      "Found convolutional layer: features.1.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.1.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.1.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.2.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.3.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.3.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.3.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.4.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.5.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.3.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.4.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.5.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.6.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.7.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.8.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.6.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.7.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.7.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.7.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n"
     ]
    }
   ],
   "source": [
    "example = torch.rand((5, 3, 32, 32))\n",
    "model = convnext_tiny()\n",
    "model.features[0][0] = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "model\n",
    "o = []\n",
    "for name, module in model.features.named_children():\n",
    "    module.register_forward_hook(lambda module, input, output: o.append(output.shape))\n",
    "model(example)\n",
    "pprint(o)\n",
    "model\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(\n",
    "            f\"Found convolutional layer: {name}, Original kernel size: {layer.kernel_size}\"\n",
    "        )\n",
    "        # Check if kernel size needs to be changed\n",
    "        if layer.kernel_size == (7, 7):\n",
    "            new_kernel_size = (4, 4)  # Define new kernel size\n",
    "            layer.kernel_size = new_kernel_size\n",
    "            print(f\"Changed kernel size to {new_kernel_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "149af392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 96, 34, 34]),\n",
       " torch.Size([5, 96, 34, 34]),\n",
       " torch.Size([5, 192, 17, 17]),\n",
       " torch.Size([5, 192, 17, 17]),\n",
       " torch.Size([5, 384, 8, 8]),\n",
       " torch.Size([5, 384, 8, 8]),\n",
       " torch.Size([5, 768, 4, 4]),\n",
       " torch.Size([5, 768, 4, 4])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a53edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNetTiny(\n",
       "  (convnet): ConvNeXt(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (3): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (4): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (5): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (6): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (7): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (8): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "          (after_skipping): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=768, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (val_acc): MulticlassAccuracy()\n",
       "  (train_acc): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2ed7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found convolutional layer: features.0.0, Original kernel size: (4, 4)\n",
      "Found convolutional layer: features.1.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.1.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.1.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.2.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.3.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.3.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.3.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.4.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.5.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.3.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.4.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.5.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.6.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.7.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.5.8.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.6.1, Original kernel size: (2, 2)\n",
      "Found convolutional layer: features.7.0.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.7.1.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Found convolutional layer: features.7.2.block.0, Original kernel size: (7, 7)\n",
      "Changed kernel size to (4, 4)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | convnet   | ConvNeXt           | 27.8 M\n",
      "1 | criterion | CrossEntropyLoss   | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "3 | train_acc | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "27.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.8 M    Total params\n",
      "111.334   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80:  48%|████▊     | 34/71 [00:06<00:06,  5.40it/s, v_num=653680, train_loss=1.350, train_acc=0.570, val_loss=1.170, val_acc=0.628]"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from src.sign_visualizer.dataset_interface import (\n",
    "    ContinualDataset,\n",
    "    ContinualDatasetConfig,\n",
    ")\n",
    "\n",
    "\n",
    "class ConvNetTiny(pl.LightningModule):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(ConvNetTiny, self).__init__()\n",
    "        self.convnet = torchvision.models.convnext_tiny()\n",
    "        self.convnet.classifier[-1] = nn.Linear(768, num_classes)\n",
    "        self.change_kernel_size_to((4, 4))\n",
    "        self.convnet.features[0][0] = nn.Conv2d(\n",
    "            3, 96, kernel_size=3, stride=1, padding=2, bias=False\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def change_kernel_size_to(self, shape):\n",
    "        for name, layer in self.convnet.named_modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                print(\n",
    "                    f\"Found convolutional layer: {name}, Original kernel size: {layer.kernel_size}\"\n",
    "                )\n",
    "                # Check if kernel size needs to be changed\n",
    "                if layer.kernel_size == (7, 7):\n",
    "                    new_kernel_size = shape  # Define new kernel size\n",
    "                    layer.kernel_size = new_kernel_size\n",
    "                    print(f\"Changed kernel size to {new_kernel_size}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        return x\n",
    "\n",
    "    def shared_step(self, batch, metric_fn):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = metric_fn(outputs, labels)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, self.train_acc)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch, self.val_acc)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        # self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        def separate_parameters(model):\n",
    "            parameters_decay = set()\n",
    "            parameters_no_decay = set()\n",
    "            modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "            modules_no_weight_decay = (nn.LayerNorm, nn.BatchNorm2d)\n",
    "\n",
    "            for m_name, m in model.named_modules():\n",
    "                for param_name, param in m.named_parameters():\n",
    "                    full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "                    if isinstance(m, modules_no_weight_decay):\n",
    "                        parameters_no_decay.add(full_param_name)\n",
    "                    elif param_name.endswith(\"bias\"):\n",
    "                        parameters_no_decay.add(full_param_name)\n",
    "                    elif isinstance(m, modules_weight_decay):\n",
    "                        parameters_decay.add(full_param_name)\n",
    "\n",
    "            # # sanity check\n",
    "            # assert len(parameters_decay & parameters_no_decay) == 0\n",
    "            # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "            return parameters_decay, parameters_no_decay\n",
    "\n",
    "        parameters_decay, parameters_no_decay = separate_parameters(self.convnet)\n",
    "        param_dict = {pn: p for pn, p in self.convnet.named_parameters()}\n",
    "        optim_groups = [\n",
    "            {\n",
    "                \"params\": [param_dict[pn] for pn in parameters_decay],\n",
    "                \"weight_decay\": 0.002,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [param_dict[pn] for pn in parameters_no_decay],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.convnet.parameters(),\n",
    "            # optim_groups,\n",
    "            lr=1e-3,\n",
    "            weight_decay=0.002,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[30, 60, 80], gamma=0.1\n",
    "        )  # Define the multi-step scheduler\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "\n",
    "class CIFAR100DataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # transforms = K.augmentation.ImageSequential(\n",
    "        #             ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.25),\n",
    "        #             RandomResizedCrop((32, 32), scale=(0.5, 1), p=0.25),\n",
    "        #             RandomRotation((-30, 30), p=0.25),\n",
    "        #             RandomHorizontalFlip(0.5),\n",
    "        #             RandomContrast((0.6, 1.8), p=0.25),\n",
    "        #             RandomSharpness((0.4, 2), p=0.25),\n",
    "        #             RandomBrightness((0.6, 1.8), p=0.25),\n",
    "        #             RandomMixUpV2(p=0.5),)\n",
    "        self.config = ContinualDatasetConfig(\"cifar100_fixed\", 5)\n",
    "        self.dataloader = partial(\n",
    "            torch.utils.data.DataLoader,\n",
    "            batch_size=128,\n",
    "            num_workers=1,\n",
    "            drop_last=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            TASK_ID = 0\n",
    "            cl_dataset = ContinualDataset(self.config)\n",
    "            self.train_dataset = cl_dataset[TASK_ID, \"train\"]\n",
    "            self.val_dataset = cl_dataset[TASK_ID, \"test\"]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader(self.train_dataset, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader(self.val_dataset)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = ConvNetTiny(num_classes=20)\n",
    "    data_module = CIFAR100DataModule()\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=100)\n",
    "    from src.datasets.data_loader import get_loaders\n",
    "\n",
    "    train_loader, val_loader, test_loader, cla = get_loaders(\n",
    "        datasets=[\"cifar100_fixed\"],\n",
    "        num_tasks=5,\n",
    "        nc_first_task=None,\n",
    "        nc_per_task=None,\n",
    "        batch_size=128,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        extra_aug=\"simclr_cifar\",\n",
    "    )\n",
    "    # trainer.fit(model, data_module)\n",
    "    task_id = 0\n",
    "    trainer.fit(model, train_loader[task_id], test_loader[task_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91740934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "111af20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bc210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
